"""
Main training script for the PGDrive project.

This script integrates all modules to run training experiments.
"""

import os
import argparse
from datetime import datetime

from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from stable_baselines3.common.vec_env import DummyVecEnv

from config import (
    FIXED_SEED, 
    FIXED_SEED_TRAINING, 
    QUICK_TEST_TRAINING,
    MULTI_SEED_TRAINING,
    TRAIN_SEEDS,
    EXPERIMENTS,
    FIXED_SEED_ENV_CONFIG,
    MULTI_SEED_ENV_CONFIG,
    set_global_seed  # ëœë¤ ì‹œë“œ ê³ ì • í•¨ìˆ˜
)
from utils.path_utils import get_model_path, get_log_path
from envs.metadrive_env import make_env
from agents.rl_agent import create_model

def train(
    total_timesteps,
    save_freq,
    model_name,
    seed,
    algorithm="ppo",
    eval_freq=25000,
    n_eval_episodes=20,
    env_config=None,
    verbose=True
):
    """
    Main training function.
    """
    # ëœë¤ ì‹œë“œ ê³ ì • (ì¬í˜„ì„± ë³´ì¥)
    set_global_seed(seed)
    
    # í™˜ê²½ ì„¤ì • ê²°ì •
    if env_config is None:
        env_config = FIXED_SEED_ENV_CONFIG
    
    print("\n" + "="*60)
    print(f"ğŸš— PGDrive Training Started: {model_name}")
    print("="*60)
    print(f"Algorithm: {algorithm.upper()}")
    print(f"Seed: {seed} (ê³ ì •ë¨ - ì¬í˜„ì„± ë³´ì¥)")
    print(f"Num Scenarios: {env_config.get('num_scenarios', 1)}")
    print(f"Total Timesteps: {total_timesteps:,}")
    print(f"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60 + "\n")

    # Create environment
    print("ğŸ“¦ Creating environment...")
    env = DummyVecEnv([make_env(seed=seed, render=False, config=env_config)])
    print("âœ… Environment created.\n")

    # Create model
    model = create_model(env, algorithm=algorithm)
    print("âœ… Model created.\n")

    # Callbacks
    checkpoint_callback = CheckpointCallback(
        save_freq=save_freq,
        save_path=os.path.dirname(get_model_path(model_name)),
        name_prefix=model_name,
        save_replay_buffer=False,
        save_vecnormalize=True,
        verbose=1
    )
    
    # Note: EvalCallbackì€ MetaDrive ì—”ì§„ ì¬ì´ˆê¸°í™” ë¬¸ì œë¡œ ì¸í•´ ë¹„í™œì„±í™”
    # í•™ìŠµ ì™„ë£Œ í›„ evaluate.pyë¡œ ë³„ë„ í‰ê°€ ê¶Œì¥

    # Start training
    print("ğŸ“ Starting training...\n")
    print("âš ï¸  Note: í•™ìŠµ ì¤‘ í‰ê°€ëŠ” MetaDrive ì œì•½ìœ¼ë¡œ ë¹„í™œì„±í™”ë¨")
    print("    í•™ìŠµ ì™„ë£Œ í›„ 'python evaluate.py'ë¡œ í‰ê°€í•˜ì„¸ìš”\n")
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=[checkpoint_callback],
            progress_bar=True,
        )
        print("\nâœ… Training finished!")

    except KeyboardInterrupt:
        print("\nâš ï¸  Training interrupted (Ctrl+C).")

    # Save final model
    final_model_path = get_model_path(model_name)
    model.save(final_model_path)
    print(f"\nğŸ’¾ Final model saved: {final_model_path}")

    # Close environment
    env.close()

    print("\n" + "="*60)
    print("ğŸ‰ Training Complete!")
    print(f"End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60 + "\n")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train an RL agent for MetaDrive.")
    
    # ì‹¤í—˜ ë°©ì‹ ì„ íƒ: --experiment ë˜ëŠ” --mode
    group = parser.add_mutually_exclusive_group()
    group.add_argument("--experiment", type=str, 
                      choices=list(EXPERIMENTS.keys()),
                      help="Pre-defined experiment to run (e.g., exp1_fixed_seed)")
    group.add_argument("--mode", type=str, default="fixed",
                      choices=["fixed", "quick", "multi"],
                      help="Training mode: fixed, quick, or multi seed")
    
    parser.add_argument("--algorithm", type=str, default="ppo", 
                       choices=["ppo", "sac", "td3"],
                       help="RL algorithm to use: PPO, SAC, or TD3.")
    args = parser.parse_args()
    
    # ëª¨ë“œë³„ ì„¤ì • ë”•ì…”ë„ˆë¦¬
    MODE_CONFIGS = {
        "fixed": {"config": FIXED_SEED_TRAINING, "seed": FIXED_SEED, "env_config": FIXED_SEED_ENV_CONFIG},
        "quick": {"config": QUICK_TEST_TRAINING, "seed": FIXED_SEED, "env_config": FIXED_SEED_ENV_CONFIG},
        "multi": {"config": MULTI_SEED_TRAINING, "seed": TRAIN_SEEDS[0], "env_config": MULTI_SEED_ENV_CONFIG},
    }
    
    # ì‹¤í—˜ í”„ë¡œí† ì½œ ì‚¬ìš© ë˜ëŠ” ëª¨ë“œ ì‚¬ìš©
    if args.experiment:
        # EXPERIMENTSì—ì„œ ì„¤ì • ë¡œë“œ
        exp = EXPERIMENTS[args.experiment]
        config = exp["train_config"]
        seed = exp["test_seeds"][0] if isinstance(exp["test_seeds"], list) else exp["test_seeds"]
        env_config = exp.get("env_config", FIXED_SEED_ENV_CONFIG)
        
        print(f"\nğŸ”¬ ì‹¤í—˜ í”„ë¡œí† ì½œ: {args.experiment}")
        print(f"ğŸ“ ì„¤ëª…: {exp['description']}\n")
    else:
        # ê¸°ì¡´ --mode ë°©ì‹
        mode_data = MODE_CONFIGS[args.mode]
        config = mode_data["config"]
        seed = mode_data["seed"]
        env_config = mode_data["env_config"]
    
    # ëª¨ë¸ëª…ì— ì•Œê³ ë¦¬ì¦˜ ë°˜ì˜
    model_name = config["model_name"].replace("ppo", args.algorithm)
    
    train(
        total_timesteps=config["total_timesteps"],
        save_freq=config["save_freq"],
        model_name=model_name,
        seed=seed,
        algorithm=args.algorithm,
        env_config=env_config
    )
